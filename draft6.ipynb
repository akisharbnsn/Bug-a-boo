{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b45dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_hint_from_llm(code: str, tier: int) -> str:\n",
    "    \"\"\"\n",
    "    Send buggy code to the LLM and ask for a tiered hint.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a debugging assistant. \n",
    "    A user will give you buggy Python code. \n",
    "    You will give tiered hints instead of the final solution:\n",
    "      - Tier 1: Very general hint (point in the right direction)\n",
    "      - Tier 2: More specific hint (narrow down likely bug area)\n",
    "      - Tier 3: Very detailed guidance\n",
    "      - Tier 4: Provide corrected working code\n",
    "    Do not jump ahead to higher tiers unless requested.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    The user has provided this buggy Python code:\n",
    "\n",
    "    {code}\n",
    "\n",
    "    Please provide the Tier {tier} hint only.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4910c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tier 1 ---\n",
      "Enter your buggy Python code (finish with blank line):\n"
     ]
    }
   ],
   "source": [
    "def interactive_debug():\n",
    "    tier = 1\n",
    "    while tier <= 4:\n",
    "        print(f\"\\n--- Tier {tier} ---\")\n",
    "        print(\"Enter your buggy Python code (finish with blank line):\")\n",
    "\n",
    "        # Collect multi-line input\n",
    "        lines = []\n",
    "        while True:\n",
    "            try:\n",
    "                line = input()\n",
    "            except EOFError:\n",
    "                break\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "            lines.append(line)\n",
    "        buggy_code = \"\\n\".join(lines)\n",
    "\n",
    "        if not buggy_code.strip():\n",
    "            print(\"No code entered. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Ask LLM for hint\n",
    "        hint = get_hint_from_llm(buggy_code, tier)\n",
    "        print(hint)\n",
    "\n",
    "        # Ask user if they want next tier\n",
    "        if tier < 4:\n",
    "            more = input(\"\\nDo you want the next hint? (yes/no): \").strip().lower()\n",
    "            if more != \"yes\":\n",
    "                break\n",
    "        tier += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_debug()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
